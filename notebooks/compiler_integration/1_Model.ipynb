{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "![Flow](img/evolution.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.export import export_qonnx\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "# Netron Port Settings\n",
    "iport=8081\n",
    "fport=8082\n",
    "\n",
    "IN_CH = 3\n",
    "OUT_CH = 128\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# set seed\n",
    "torch.manual_seed(0)\n",
    "inp = torch.randn(BATCH_SIZE, IN_CH)\n",
    "\n",
    "linear_path = 'linear_qonnx.onnx'\n",
    "qlinear_path = 'quant_linear_qonnx.onnx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1, Pytorch model\n",
    "\n",
    "create and export a pytorch model to standard onnx graph, in this example we have a GEMM node for the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'linear_qonnx.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7041f54910>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Sequential(\n",
    "    nn.Linear(IN_CH, OUT_CH, bias=True),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "exported_model = export_qonnx(linear, args=inp, export_path=linear_path, opset_version=13)\n",
    "showInNetron(linear_path,localhost_url=\"localhost\", port=iport, forwarded_port=fport)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2, Quantize\n",
    "\n",
    "Convert the model to a quantized mode, see Brevitas documentation on QAT or PTQ methods for training. \n",
    "the Brevitas API replaces the Pytorch NN APIs with the addition of bit width parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'quant_linear_qonnx.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7041ab75b0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "qlinear = nn.Sequential(\n",
    "    qnn.QuantIdentity(bit_width=4, return_quant_tensor=True),\n",
    "    qnn.QuantLinear(IN_CH, OUT_CH, bias=True, weight_bit_width=4),\n",
    "    qnn.QuantReLU(bit_width=4)\n",
    ")\n",
    "\n",
    "# qlinear = qnn.QuantLinear(IN_CH, OUT_CH, bias=True, weight_bit_width=4)\n",
    "exported_model = export_qonnx(qlinear, args=inp, export_path=qlinear_path, opset_version=13)\n",
    "\n",
    "showInNetron(qlinear_path,localhost_url=\"localhost\", port=iport, forwarded_port=fport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to feed this model into FINN.\n",
    "\n",
    "# QONNX To FINN ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/azizb/brainwave/finn_workshop/finn-core/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 13. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'qonnx_2_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f70400fd0c0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.util.cleanup import cleanup_model\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(qlinear_path)\n",
    "inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(inp_name, DataType[\"UINT4\"])\n",
    "model = cleanup_model(model)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "        \n",
    "model.save(\"qonnx_2_finn.onnx\")\n",
    "\n",
    "showInNetron(\"qonnx_2_finn.onnx\",localhost_url=\"localhost\", port=iport, forwarded_port=fport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamline \n",
    "\n",
    "It is possible at this point to run CPP Sim to generate reference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'prep.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/azizb/brainwave/finn_workshop/finn-core/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:136: UserWarning: Assuming 2D input is NC\n",
      "  warnings.warn(\"Assuming 2D input is NC\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7041aef460>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.transformation.general import GiveUniqueNodeNames\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from finn.builder.build_dataflow_steps import step_streamline\n",
    "\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "\n",
    "cfg = build_cfg.DataflowBuildConfig(\n",
    "    verbose=True,\n",
    "    output_dir = \"output\",\n",
    "    fpga_part=\"xcvm1802-vsvd1760-2MP-e-S\",\n",
    "    synth_clk_period_ns=3.0,\n",
    "    generate_outputs=[],\n",
    "    standalone_thresholds=True\n",
    ")\n",
    "\n",
    "\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "\n",
    "model = step_streamline(model, cfg)\n",
    "\n",
    "model.save(\"prep.onnx\")\n",
    "\n",
    "netron.stop((\"0.0.0.0\", iport))\n",
    "showInNetron(\"prep.onnx\",localhost_url=\"localhost\", port=iport, forwarded_port=fport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'infer_threshold2.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7041a7ebf0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.convert_to_hw_layers import InferThresholdingLayer\n",
    "\n",
    "model = model.transform(InferThresholdingLayer())\n",
    "model.save(\"infer_threshold.onnx\")\n",
    "netron.stop((\"0.0.0.0\", iport))\n",
    "showInNetron(\"infer_threshold.onnx\",localhost_url=\"localhost\", port=iport, forwarded_port=fport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all OPS to FINN HW Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'step_convert_to_hw.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8082/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7041a7cd00>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.builder.build_dataflow_steps import step_convert_to_hw\n",
    "\n",
    "model = step_convert_to_hw(model,cfg)\n",
    "model.save(\"step_convert_to_hw.onnx\")\n",
    "netron.stop((\"0.0.0.0\", iport))\n",
    "showInNetron(\"step_convert_to_hw.onnx\",localhost_url=\"localhost\", port=iport, forwarded_port=fport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specialize Layers\n",
    "\n",
    "Select RTL module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
